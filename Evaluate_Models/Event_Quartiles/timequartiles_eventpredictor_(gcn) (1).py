# -*- coding: utf-8 -*-
"""TimeQuartiles_EventPredictor_(GCN).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jqC7fi98WJcLHJuNNbo4YuC31LRTap-C

<a href="https://colab.research.google.com/github/ishwarvenugopal/GCN-ProcessPrediction/blob/master/Evaluate_Models/Time_Quartiles/TimeQuartiles_EventPredictor_(GCN).ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>

# Importing necessary packages and functions
"""

# !pip install pm4py

import pandas as pd
import numpy as np
import time
from datetime import datetime
import numpy as np
from numpy import vstack
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import LabelEncoder
from scipy.linalg import fractional_matrix_power

import torch 
import torch.nn as nn
from torch.nn import Parameter
#from torch_geometric.nn.inits import glorot, zeros

from torch.utils.data import Dataset
from torch.utils.data import random_split
from torch.utils.data import DataLoader
from torch.utils.data import Subset
from sklearn.model_selection import train_test_split

from pm4py.objects.conversion.log import converter as log_converter
from pm4py.algo.discovery.dfg import algorithm as dfg_algorithm
from pm4py.objects.conversion.dfg import converter as dfg_conv
from pm4py.objects.log.importer.xes import importer as xes_importer
from pm4py.visualization.dfg import visualizer as dfg_vis_fact
from pm4py.visualization.petrinet import visualizer as pn_vis

# from torch_geometric.nn.inits import glorot, zeros 

#Unable to import above line, so manually copy-pasting the source code

import math

def glorot(tensor):
    if tensor is not None:
        stdv = math.sqrt(6.0 / (tensor.size(-2) + tensor.size(-1)))
        tensor.data.uniform_(-stdv, stdv)

def zeros(tensor):
    if tensor is not None:
        tensor.data.fill_(0)

import bisect
import warnings
from torch._utils import _accumulate
from torch import randperm, default_generator

class Subset(Dataset):
    r"""
    Subset of a dataset at specified indices.

    Arguments:
        dataset (Dataset): The whole Dataset
        indices (sequence): Indices in the whole set selected for subset
    """
    def __init__(self, dataset, indices):
        self.dataset = dataset
        self.indices = indices

    def __getitem__(self, idx):
        return self.dataset[self.indices[idx]]

    def __len__(self):
        return len(self.indices)
        
def random_split(dataset, lengths, generator=default_generator):
    r"""
    Randomly split a dataset into non-overlapping new datasets of given lengths.
    Optionally fix the generator for reproducible results, e.g.:

    >>> random_split(range(10), [3, 7], generator=torch.Generator().manual_seed(42))

    Arguments:
        dataset (Dataset): Dataset to be split
        lengths (sequence): lengths of splits to be produced
        generator (Generator): Generator used for the random permutation.
    """
    if sum(lengths) != len(dataset):
        raise ValueError("Sum of input lengths does not equal the length of the input dataset!")

    indices = randperm(sum(lengths), generator=generator).tolist()
    return [Subset(dataset, indices[offset - length : offset]) for offset, length in zip(_accumulate(lengths), lengths)]

"""# Setting the parameters"""

# Helpdesk dataset

#path = '../../Data/helpdesk.csv'
#dataset = 'helpdesk'
#save_folder = '../../python_files/Results/helpdesk'
#num_nodes = 9

# # BPI dataset

path = '../../Data/bpi_12.csv'
dataset = 'bpi'
save_folder = '../../python_files/Results/bpi'
num_nodes = 6 

num_features = 4
showProcessGraph = False
device = 'cuda' if torch.cuda.is_available() else 'cpu'
# device = 'cuda'
num_epochs = 100
seed_value = 42

weighted_adjacency = True
binary_adjacency = False
laplacian_matrix = True
variant = 'LaplasianWeighted'
num_runs = 1

lr_optimum = 1e-04 
# lr_optimum = None  #if evaluation is for all three learning rates

if lr_optimum == 1e-03:
    lr_init = 0
    lr_run_end = lr_init + 1
elif lr_optimum == 1e-04:
    lr_init = 1
    lr_run_end = lr_init + 1
elif lr_optimum == 1e-05:
    lr_init = 2
    lr_run_end = lr_init + 1
elif lr_optimum == None:
    lr_init = 0
    lr_run_end = 3

"""# Data Pre-processing"""

def generate_features_with_quartiles (df,total_activities,num_features):
  lastcase = ''
  firstLine = True
  numlines = 0
  casestarttime = None
  lasteventtime = None
  features = []

  for i,row in df.iterrows():
    t = time.strptime(row[2], "%Y-%m-%d %H:%M:%S")
    if row[0]!=lastcase:
        casestarttime = t
        lasteventtime = t
        lastcase = row[0]
        numlines+=1
    timesincelastevent = datetime.fromtimestamp(time.mktime(t))-datetime.fromtimestamp(time.mktime(lasteventtime))
    timesincecasestart = datetime.fromtimestamp(time.mktime(t))-datetime.fromtimestamp(time.mktime(casestarttime))
    midnight = datetime.fromtimestamp(time.mktime(t)).replace(hour=0, minute=0, second=0, microsecond=0)
    timesincemidnight = datetime.fromtimestamp(time.mktime(t))-midnight
    timediff = 86400 * timesincelastevent.days + timesincelastevent.seconds
    timediff2 = 86400 * timesincecasestart.days + timesincecasestart.seconds
    timediff3 = timesincemidnight.seconds #this leaves only time even occured after midnight
    timediff4 = datetime.fromtimestamp(time.mktime(t)).weekday() #day of the week
    lasteventtime = t
    firstLine = False
    feature_list = [timediff,timediff2,timediff3,timediff4]
    features.append(feature_list)

  df['Feature Vector'] = features
  
  firstLine = True
  case_lengths = []
  start_pos = 0

  for i,row in df.iterrows():
    if not firstLine:
      if (row[3][1]==0):
        end_pos = i-1
        for j in range(start_pos,end_pos):
          case_lengths[j]= current_case_length
        start_pos = i
        current_case_length = row[3][1]
        case_lengths.append(current_case_length)
      else:
        current_case_length = row[3][1]
        case_lengths.append(current_case_length)
    else:
      current_case_length = row[3][1]
      case_lengths.append(current_case_length)
      firstLine = False

  end_pos = len(case_lengths) - 1
  for j in range(start_pos,end_pos):
    case_lengths[j]= current_case_length

  df['Case_Length'] = case_lengths

  quartiles = []

  for i,row in df.iterrows():
    if (row[4]!=0):
      temp = float(row[3][1])/ float(row[4])
    else:
      temp = 1
    temp = temp*4.0
    if temp<=1:
      quartiles.append(1)
    elif (temp>1) and (temp<=2):
      quartiles.append(2)
    elif (temp>2) and (temp<=3):
      quartiles.append(3)
    elif (temp>3) and (temp<=4):
      quartiles.append(4)

  df['Quartile'] = quartiles

  firstLine = True
  NN_features =[]

  for i,row in df.iterrows():
    if firstLine:
      features = np.zeros((total_activities,num_features))
      features[row[1] - 1] = row[3]
      firstLine = False
    else:
      if (row[3][0] == 0):
        features = np.zeros((total_activities,num_features))
        features[row[1] - 1] = row[3]
      else:
        features = np.copy(prev_row_features)
        features[row[1] - 1] = row[3]
    prev_row_features = features
    NN_features.append([row['Quartile'],features])  
  
  return NN_features

def generate_labels(df,total_activities):
  next_activity = []
  next_timestamp = []

  for i,row in df.iterrows():
    if (i != 0):
      if (row[3][0]==0):
        next_activity.append(total_activities)
      else:
        next_activity.append(row[1]-1)
  next_activity.append(total_activities)
  for i,row in df.iterrows():
    if (i != 0):
      if (row[3][0]==0):
        next_timestamp.append(0)
      else:
        next_timestamp.append(row[3][0])
  next_timestamp.append(0)

  return next_activity,next_timestamp

class EventLogData(Dataset):
  def __init__ (self, input, output):
    self.X = input
    self.y = output
    self.y = self.y.to(torch.float32)
    self.y = self.y.reshape((len(self.y),1))

  #get the number of rows in the dataset
  def __len__(self):
    return len(self.X)

  #get a row at a particular index in the dataset
  def __getitem__ (self,idx):
    return [self.X[idx],self.y[idx]]
  
  # get the indices for the train and test rows
  def get_splits(self, n_test = 0.33, n_valid = 0.2):
    train_idx,test_idx = train_test_split(list(range(len(self.X))),test_size = n_test, shuffle = False )
    train_idx, valid_idx = train_test_split(train_idx, test_size = n_valid, shuffle = True)
    train = Subset(self, train_idx)
    valid = Subset(self, valid_idx)
    test = Subset(self, test_idx)
    return train, valid, test

def prepare_data_for_Predictor(NN_features,label):
  dataset = EventLogData(NN_features,label)
  train, valid, test = dataset.get_splits()
  train_dl = DataLoader(train, batch_size=1, shuffle = True)
  valid_dl = DataLoader(valid, batch_size=1, shuffle = False)
  test_dl = DataLoader(test, batch_size = 1, shuffle = False)
  return train_dl, valid_dl, test_dl

def generate_input_and_labels (path):
  df = pd.read_csv(path)
  total_unique_activities = num_nodes
  NN_features = generate_features_with_quartiles(df,total_unique_activities,num_features)
  next_activity, next_timestamp = generate_labels(df,total_unique_activities)
  # NN_features = torch.Tensor(NN_features).to(torch.float32)
  next_activity = torch.Tensor(next_activity).to(torch.float32)
  next_timestamp = torch.Tensor(next_timestamp).to(torch.float32)
 
  train_dl, valid_dl, test_dl = prepare_data_for_Predictor(NN_features, next_activity)
  #train_dl, test_dl = prepare_data_for_Predictor(NN_features, next_timestamp)
  
  return train_dl,valid_dl,test_dl

"""# Getting Adjacency Matrix from Process Graph"""

def generate_process_graph (path):
  data = pd.read_csv(path)
  num_nodes = data['ActivityID'].nunique() # 9 for helpdesk.csv
  cols = ['case:concept:name','concept:name','time:timestamp']
  data.columns = cols 
  data['time:timestamp'] = pd.to_datetime(data['time:timestamp'])
  data['concept:name'] = data['concept:name'].astype(str)
  log = log_converter.apply(data, variant=log_converter.Variants.TO_EVENT_LOG)
  dfg = dfg_algorithm.apply(log)
  if showProcessGraph:
    visualize_process_graph(dfg,log)
  max = 0
  min = 0
  adj = np.zeros((num_nodes,num_nodes))
  for k,v in dfg.items():
    for i in range(num_nodes):
      if(k[0] == str(i+1)):
        for j in range(num_nodes):
          if (k[1] == str(j+1)):
            adj[i][j] = v
            if (v > max): max=v
            if (v< min): min=v

  # print("Raw weighted adjacency matrix: \n {}".format(adj))
  
  if binary_adjacency:
    for i in range(num_nodes):
      for j in range(num_nodes):
        if (adj[i][j]!=0):
          adj[i][j]=1
    # print("Binary adjacency matrix: \n {}".format(adj))
  
  D = np.array(np.sum(adj, axis=1))
  D = np.matrix(np.diag(D))
  # print("Degree matrix: \n {}".format(adj))
  
  adj = np.matrix(adj)

  if laplacian_matrix:
    adj = D - adj # Laplacian Transform 
    # print("Laplacian matrix: \n {}".format(adj))

  # adj = (D**-1)*adj
  adj = fractional_matrix_power(D, -0.5)*adj*fractional_matrix_power(D, -0.5)
  adj = torch.Tensor(adj).to(torch.float)
  
  # print("Symmetrically normalised Adjacency matrix: \n {}".format(adj))
  
  return adj

def visualize_process_graph (dfg,log):
  dfg_gv = dfg_vis_fact.apply(dfg, log, parameters={dfg_vis_fact.Variants.FREQUENCY.value.Parameters.FORMAT: "jpeg"})
  dfg_vis_fact.view(dfg_gv)
  dfg_vis_fact.save(dfg_gv,"dfg.jpg")

"""# Building Model"""

class GCNConv(torch.nn.Module):
    def __init__(self, num_nodes, num_features, out_channels):
        super(GCNConv, self).__init__()

        self.in_channels = num_features
        self.out_channels = out_channels

        self.weight = Parameter(torch.Tensor(num_features, out_channels))
        self.bias = Parameter(torch.Tensor(num_nodes))

        self.reset_parameters()

    def reset_parameters(self):
        glorot(self.weight)
        zeros(self.bias)

    def forward(self, x, adj):
        x = adj@x@self.weight
        x = torch.flatten(x)
        x = x + self.bias
        return x

class GraphAttentionLayer(nn.Module):
    def __init__(self, in_features, out_features, concat=True):
        super(GraphAttentionLayer, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.concat = concat
        self.bias = Parameter(torch.Tensor(num_nodes))

        #print(in_features)
        #print(out_features)
        self.W = Parameter(torch.Tensor(in_features, out_features))
        glorot(self.W)
        self.a = Parameter(torch.Tensor((2*out_features), 1))
        zeros(self.a)
        zeros(self.bias)

        self.leakyrelu = nn.LeakyReLU()

    def forward(self, h, adj):
        Wh = torch.mm(h, self.W) # h.shape: (N, in_features), Wh.shape: (N, out_features)
        a_input = self._prepare_attentional_mechanism_input(Wh)
        #print(a_input.size())
        e = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(2))
        #print(e.size())
        zero_vec = torch.zeros_like(e)
        #print(zero_vec.size())
        
        attention = torch.where(adj > 0, e, zero_vec)
        #print(attention.size())
        attention = torch.nn.functional.softmax(attention, dim=1)
        h_prime = torch.matmul(attention, Wh)
        #print(h_prime.size())
        x = torch.flatten(h_prime)
        return x

    def _prepare_attentional_mechanism_input(self, Wh):
        N = Wh.size()[0] # number of nodes

        # Below, two matrices are created that contain embeddings in their rows in different orders.
        # (e stands for embedding)
        # These are the rows of the first matrix (Wh_repeated_in_chunks): 
        # e1, e1, ..., e1,            e2, e2, ..., e2,            ..., eN, eN, ..., eN
        # '-------------' -> N times  '-------------' -> N times       '-------------' -> N times
        # 
        # These are the rows of the second matrix (Wh_repeated_alternating): 
        # e1, e2, ..., eN, e1, e2, ..., eN, ..., e1, e2, ..., eN 
        # '----------------------------------------------------' -> N times
        # 
        
        Wh_repeated_in_chunks = Wh.repeat_interleave(N, dim=0)
        Wh_repeated_alternating = Wh.repeat(N, 1)
        # Wh_repeated_in_chunks.shape == Wh_repeated_alternating.shape == (N * N, out_features)

        # The all_combination_matrix, created below, will look like this (|| denotes concatenation):
        # e1 || e1
        # e1 || e2
        # e1 || e3
        # ...
        # e1 || eN
        # e2 || e1
        # e2 || e2
        # e2 || e3
        # ...
        # e2 || eN
        # ...
        # eN || e1
        # eN || e2
        # eN || e3
        # ...
        # eN || eN

        all_combinations_matrix = torch.cat([Wh_repeated_in_chunks, Wh_repeated_alternating], dim=1)
        # all_combinations_matrix.shape == (N * N, 2 * out_features)

        return all_combinations_matrix.view(N, N, 2 * self.out_features)


class EventPredictor(torch.nn.Module):
    def __init__(self,num_nodes, num_features = 4):
        super(EventPredictor, self).__init__()
        self.layer1 = GraphAttentionLayer(num_features, out_features=2)
        #self.layer1 = GCNConv(num_nodes, num_features, out_channels=1)
        self.layer2 = nn.Sequential(
            nn.Linear(2*num_nodes,256),
            nn.Tanh(),
            nn.Linear(256,256),
            nn.Tanh(),
            nn.Linear(256,num_nodes+1),
        )
        

    def forward(self, x, adj):
        x = self.layer1(x,adj)
        #print("asdasda ")
        #print(x.size())
        x = self.layer2(x)
        # x = torch.sigmoid(x)
        return x


"""# Evaluate Model"""

def evaluate_model():
  predictions_1, actuals_1 = list(),list()
  predictions_2, actuals_2 = list(),list()
  predictions_3, actuals_3 = list(),list()
  predictions_4, actuals_4 = list(),list()
  predictions_all, actuals_all = list(),list()

  with torch.no_grad():
    model.eval()

    for i,(inputs,targets) in enumerate(test_dl):
      
      input_quartile = int(inputs[0][0])
      inputs = inputs[1].to(torch.float32)
      
      inputs,targets = inputs.to(device),targets.to(device)

      yhat = model(inputs[0],adj)
      #Retrieving predictions and true values as numpy arrays
      yhat = yhat.to('cpu')
      yhat = torch.argmax(yhat)
      actual = targets.to('cpu')
      actual = actual[0]
      predictions_all.append(yhat)
      actuals_all.append(actual)

      if (input_quartile==1):
        predictions_1.append(yhat)
        actuals_1.append(actual)
      elif (input_quartile==2):
        predictions_2.append(yhat)
        actuals_2.append(actual)
      elif (input_quartile==3):
        predictions_3.append(yhat)
        actuals_3.append(actual)
      elif (input_quartile==4):
        predictions_4.append(yhat)
        actuals_4.append(actual)
      
  #Calculating the accuracy
  acc_1 = accuracy_score(actuals_1, predictions_1)
  acc_2 = accuracy_score(actuals_2, predictions_2)
  acc_3 = accuracy_score(actuals_3, predictions_3)
  acc_4 = accuracy_score(actuals_4, predictions_4)
  acc_all = accuracy_score(actuals_all, predictions_all)

  return acc_1,acc_2,acc_3,acc_4,acc_all

accuracies = {}
lr_run = lr_init
for lr_run in range(lr_run, lr_run_end):
  if lr_run==0:
    lr_value = 1e-03
  elif lr_run==1:
    lr_value = 1e-04
  elif lr_run==2:
    lr_value = 1e-05
  model = EventPredictor(num_nodes, num_features)
  train_dl,valid_dl,test_dl = generate_input_and_labels(path)
  adj = generate_process_graph(path)
  # criterion = nn.CrossEntropyLoss()
  # optimizer = torch.optim.Adam(model.parameters(),lr=lr_value)
  # print("************* Event Predictor ***************")
  # print("Train size: {}, Validation size:{}, Test size: {}".format(len(train_dl.dataset),len(valid_dl.dataset),len(test_dl.dataset)))
  # print(model)
  model = model.to(device)
  adj = adj.to(device)
  run = 0

  quartile_1 = []
  quartile_2 = []
  quartile_3 = []
  quartile_4 = []
  overall = []

  for run in range(num_runs):
    model.load_state_dict(torch.load('D:\\GraphAnalytics\\GCN-ProcessPrediction-master\\python_files\\Results\\bpi\\EventPredictor_parameters_BPI_laplacianOnWeighted_0.0001_run0_GATTTT.pt'))
    acc_1,acc_2,acc_3,acc_4,acc_all = evaluate_model()
    print("Accuracy for Quartile 1: {}".format(acc_1))
    print("Accuracy for Quartile 2: {}".format(acc_2))
    print("Accuracy for Quartile 3: {}".format(acc_3))
    print("Accuracy for Quartile 4: {}".format(acc_4))
    print("Overall Accuracy: {}".format(acc_all))
    quartile_1.append(acc_1)
    quartile_2.append(acc_2)
    quartile_3.append(acc_3)
    quartile_4.append(acc_4)
    overall.append(acc_all)

  compile = [quartile_1,quartile_2,quartile_3,quartile_4,overall]
  accuracies[lr_value] = compile

print("Dataset: {}".format(dataset))
print("Model Variant: {}".format(variant))

data = []
lr_run = lr_init
for lr_run in range(lr_run, lr_run_end):
  if lr_run==0:
    lr_value = 1e-03
  elif lr_run==1:
    lr_value = 1e-04
  elif lr_run==2:
    lr_value = 1e-05
  q1_avg = np.round(np.mean(np.array(accuracies[lr_value][0])),4)
  q1_std = np.round(np.std(np.array(accuracies[lr_value][0])),4)
  q2_avg = np.round(np.mean(np.array(accuracies[lr_value][1])),4) 
  q2_std = np.round(np.std(np.array(accuracies[lr_value][1])),4)
  q3_avg = np.round(np.mean(np.array(accuracies[lr_value][2])),4) 
  q3_std = np.round(np.std(np.array(accuracies[lr_value][2])),4)
  q4_avg = np.round(np.mean(np.array(accuracies[lr_value][3])),4) 
  q4_std = np.round(np.std(np.array(accuracies[lr_value][3])),4)
  qall_avg = np.round(np.mean(np.array(accuracies[lr_value][4])),4) 
  qall_std = np.round(np.std(np.array(accuracies[lr_value][4])),4)
  
  print("Learning rate: {}, Overall Accuracy: {}".format(lr_value,qall_avg))
  row = [variant, lr_value, '{} +/- {}'.format(q1_avg,q1_std),'{} +/- {}'.format(q2_avg,q2_std),'{} +/- {}'.format(q3_avg,q3_std),'{} +/- {}'.format(q4_avg,q4_std),'{} +/- {}'.format(qall_avg,qall_std)]  
  data.append(row)

columns = ['Variant','Learning Rate', 'Quartile_1','Quartile_2','Quartile_3','Quartile_4','Overall']
df = pd.DataFrame(data,columns=columns)
df.to_csv('TimeQuartiles_Accuracy_{}_{}.csv'.format(dataset,variant),index=False)
df

